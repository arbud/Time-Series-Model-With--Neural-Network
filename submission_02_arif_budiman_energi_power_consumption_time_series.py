# -*- coding: utf-8 -*-
"""Submission 02 Arif Budiman - Energi Power Consumption Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S5Mc3Xs2iy3b6cpojqGd7Ef7r8pZt30D

# **Pendahuluan**

Pertama-tama dalam proyek ini saya menggunakan dataset yang diperoleh dari [kaggle](https://www.kaggle.com/) yaitu data [Electric Power Consumption](https://www.kaggle.com/datasets/fedesoriano/electric-power-consumption).

Data dari link kaggel tersebut akan dicopy API nya untuk diambil datanya berikut API dari Electric Power Consumption : (kaggle datasets download -d fedesoriano/electric-power-consumption)

Berikut API Token yang diperoleh dari kaggle : [Kaggle API Token](https://drive.google.com/file/d/1OnxF8SKCfAsk5b2iBSU2v3NoIvWdwZjZ/view?usp=sharing)

# **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn
import time
from tensorflow import keras
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Bidirectional
from keras.layers import LSTM
from keras.layers import Dropout

"""# **Menarik Data Dari Kaggle Dengan API Command**

### Berikut cara menarik dataset yang bersumber dari kaggle :

*   Klik ikon folder disebelah kiri *sidebar* google colab
*   Klik kanan dan pilih *'New Folder'* lalu beri nama folder tersebut dengan nama kaggle
*   Klik folder yang sudah dibuat tadi dan kemudian klik kanan lalu pilih *upload* dan pilih file **kaggle.json** (untuk memperoleh file kaggle.json ini harus login ke akun kaggle yang anda miliki lalu pilih setting dan pada menu setting pilih API dan create token maka akan langsung terunduh file kaggle.json)

Jika sudah maka selanjutnya mengikuti cara seperti dibawah berikut
"""

!pip install kaggle

!mkdir -p ~/.kaggle
!cp kaggle/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d fedesoriano/electric-power-consumption

!unzip electric-power-consumption.zip

df_electric = pd.read_csv('/content/powerconsumption.csv')

df_electric

df_electric['Datetime'] = pd.to_datetime(df_electric['Datetime'])

# Cek apakah ada data yang null
df_electric.isnull().sum()

# Cek info datasetnya
df_electric.info()

# Ubah data pada kolom tanggal kedalam bentuk array
tgl = df_electric['Datetime'].values
norm_tgl = tgl.reshape(-1,1)
norm_tgl

# Ubah data pada kolom humidity kedalam bentuk array
hum = df_electric['Humidity'].values.reshape(-1,1)
hum

# Normalisasikan data pada humidity dengan menggunakan StandardScaler
scaler = StandardScaler()
normalized_hum = scaler.fit_transform(hum)
print(normalized_hum)

# jadikan array data humidity yang sudah di normalisasikan
normalized_hum_array = normalized_hum.flatten()
norm_hum = normalized_hum_array.reshape(-1,1)
norm_hum

# Visualisasikan data tanggal dan humidity dengan menggunakan matplotlib
plt.figure(figsize=(15, 5))
plt.plot(norm_tgl, norm_hum)
plt.title('Electric Power Consumption Humidity',
          fontsize=20);

"""# **Train Test Split Data**

Untuk pembagian datanya adalah validation set 20% dari total dataset
"""

tgl_train, tgl_val, hum_train, hum_val = train_test_split(norm_tgl, norm_hum, test_size=0.2, shuffle=False)

# Mengetahui total data train dan validasi

print('total tgl_train : ', len(tgl_train))
print('total tgl_val : ',len(tgl_val))
print('total hum_train : ', len(hum_train))
print('total hum_val : ',len(hum_val))

"""# **Threshold MAE**

model memiliki nilai MAE < 10% skala data, untuk mengetahuinya dapat dilakukan dengan kode berikut:
"""

# Mengetahui nilai terbesar dan terkecil dari data

print('Data Maksimal StandarScaler Humidity :', norm_hum.max())
print('Data Minimal StandarScaler Humidity :',norm_hum.min())

threshold_mae = (norm_hum.max() - norm_hum.min()) * 10/100
threshold_mae

"""# **Fungsi untuk mengubah data agar formatnya dapat diterima oleh model**"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)  # Menambahkan dimensi terakhir
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1]))  # Menyesuaikan bentuk output
    return ds.batch(batch_size).prefetch(1)

"""# **Model Time Series**"""

# buat arsitektur model dengan Bidirectional LSTM

train_set = windowed_dataset(hum_train, window_size=100, batch_size=200, shuffle_buffer=5000)
val_set = windowed_dataset(hum_val, window_size=100, batch_size=200, shuffle_buffer=5000)

model_epc = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(None, 1)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1),
])

# buat optimizer,loss, dan metrics menggunakan MAE pada model
optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)

model_epc.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=['mae'])

# buat callback untuk mengetahui jika MAE sesuai dengan Threshold yang ditentukan
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')< threshold_mae):
      print("\nMean Absolute Error (MAE) telah mencapai <10% skala data!")
      self.model.stop_training = True
callbacks = myCallback()

# ujicoba model yang telah dibuat di atas
start = time.time()
history_epc = model_epc.fit(train_set, validation_data=val_set, batch_size=128, epochs=100, verbose=2, callbacks=[callbacks])

stop = time.time()
print(f"Lama Waktu Training yang Dibutuhkan: {round((stop - start)/60)}minute")

# Buat Plot loss dan MAE nya
acc = history_epc.history['mae']
val_acc = history_epc.history['val_mae']

loss = history_epc.history['loss']
val_loss = history_epc.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training MAE')
plt.plot(val_acc, label='Validation MAE')
plt.legend(loc='lower right')
plt.ylabel('MAE')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation MAE')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('MAE')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()